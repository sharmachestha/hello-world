{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Code for inferring missing promos from sellout",
   "id": "2489fc31ffdcf2ce"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import pandas as pd\n",
    "from pandas.api.indexers import FixedForwardWindowIndexer"
   ],
   "id": "6961656f18962b7d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "RAW_AVERAGE_PRICE = \"raw_average_price\"  # value / volume in the sellout\n",
    "RETAIL_STANDARD_PRICE = \"retail_standard_price\"  # this is per hL\n",
    "PROMO_DEPTH_VALUE = \"promo_depth_value\"\n",
    "PROMO_MECHANIC = \"promo_mechanic\"\n",
    "PROMO_EVENT_NAME = \"promo_event_name\"\n",
    "VOLUME = \"total_volume\"\n",
    "PROMO_VOLUME = \"promo_volume\"\n",
    "PROMO_ON = \"promo_on\"  # indicates if there's at least one promo days during the week\n",
    "START_DATE_WEEK = \"start_date_week\"\n",
    "UNIVERSE_COLS = [\"product_family\", \"customer\"]\n",
    "PROMO_START_DATE = \"promo_start_date\"\n",
    "PROMO_END_DATE = \"promo_end_date\""
   ],
   "id": "db19a5efabf49058"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def _combine_change_points(upward_change_points: pd.Series, downward_change_points: pd.Series, calibration_points: pd.Series) -> pd.Series:\n",
    "    \"\"\"Combine the three types of change points to a single vector with invalid changes removed.\n",
    "\n",
    "    Invalid changes: upward change that is lower than the previous point or vv\n",
    "\n",
    "    Args:\n",
    "        upward_change_points: The upward change points.\n",
    "        downward_change_points: The downward change points.\n",
    "        calibration_points: The calibration points.\n",
    "\n",
    "    Returns:\n",
    "        A series containing the combined change points.\n",
    "    \"\"\"\n",
    "    change_points = calibration_points.combine_first(downward_change_points).combine_first(upward_change_points)\n",
    "\n",
    "    diff_since_last_value = change_points.dropna().diff().reindex_like(change_points)\n",
    "\n",
    "    invalid_upward = upward_change_points.notna() & (diff_since_last_value < 0)\n",
    "    invalid_downward = downward_change_points.notna() & (diff_since_last_value > 0)\n",
    "\n",
    "    if invalid_upward.any() or invalid_downward.any():\n",
    "        # iteratively remove invalid change points\n",
    "        while invalid_upward.any() or invalid_downward.any():\n",
    "            change_points = change_points.where(~invalid_upward).where(~invalid_downward)\n",
    "\n",
    "            diff_since_last_value = change_points.dropna().diff().reindex_like(change_points)\n",
    "\n",
    "            invalid_upward = upward_change_points.notna() & (diff_since_last_value < 0)\n",
    "            invalid_downward = downward_change_points.notna() & (diff_since_last_value > 0)\n",
    "\n",
    "    return change_points"
   ],
   "id": "f288116553b477a2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def _select_first_nonmissing(series_in: pd.Series) -> pd.Series:\n",
    "    \"\"\"Select the first non-missing value of each sequence of non-missing values.\n",
    "\n",
    "    Args:\n",
    "        series_in: The series to select the first non-missing value from.\n",
    "\n",
    "    Returns:\n",
    "        A series containing the first non-missing value of each sequence of non-missing values.\n",
    "    \"\"\"\n",
    "    return series_in.where(series_in.shift(1).isna())"
   ],
   "id": "2dfa5e7493cb5535"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def _get_change_points(price: pd.Series, forward_looking: pd.Series, backward_looking: pd.Series) -> pd.Series:\n",
    "    \"\"\"Get change points by comparing forward and backward looking prices.\n",
    "\n",
    "    Args:\n",
    "        price: The non-promo price.\n",
    "        forward_looking: The forward looking price.\n",
    "        backward_looking: The backward looking price.\n",
    "\n",
    "    Returns:\n",
    "        A series containing the change points.\n",
    "    \"\"\"\n",
    "    # change up: first point where forward > backward and price >> backward\n",
    "    upward_condition = (forward_looking > backward_looking) & (price > ((backward_looking + forward_looking) / 2))\n",
    "    upward_change_points = forward_looking.where(upward_condition).pipe(_select_first_nonmissing)\n",
    "\n",
    "    # change down: first point where forward < backward and price << forward\n",
    "    downward_condition = (forward_looking < backward_looking) & (price < ((backward_looking + forward_looking) / 2))\n",
    "    downward_change_points = forward_looking.where(downward_condition).pipe(_select_first_nonmissing)\n",
    "\n",
    "    # if forward and backward looking curves are the same, that should be a calibration\n",
    "    calibration_points = forward_looking.where(forward_looking == backward_looking).pipe(_select_first_nonmissing)\n",
    "\n",
    "    return _combine_change_points(upward_change_points, downward_change_points, calibration_points)\n"
   ],
   "id": "fb5642d35585eb84"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def _connect_change_points(change_points: pd.Series, forward_looking: pd.Series) -> pd.Series:\n",
    "    \"\"\"Connect change points by adding a first point and ffilling between points.\n",
    "\n",
    "    Args:\n",
    "        change_points: The change points.\n",
    "        forward_looking: The forward looking price.\n",
    "\n",
    "    Returns:\n",
    "        A series containing the connected change points.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        change_points.iat[0] = forward_looking.dropna().iat[0]\n",
    "        smoothed_price = change_points.ffill()\n",
    "    except IndexError:\n",
    "        # All prices are NaN\n",
    "        smoothed_price = change_points\n",
    "\n",
    "    return smoothed_price"
   ],
   "id": "ec4805d6670886e3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def _infer_rsp_from_non_promo_price(price: pd.Series, window_length: int, accepted_outliers: int) -> pd.Series:\n",
    "    \"\"\"Infer retail standard price from the non-promo price.\n",
    "\n",
    "    Args:\n",
    "        price: The non-promo price.\n",
    "        window_length: The number of weeks to look back to infer the retail standard price.\n",
    "        accepted_outliers: The number of outliers to accept when inferring the retail standard price.\n",
    "\n",
    "    Returns:\n",
    "        A series containing the retail standard price.\n",
    "    \"\"\"\n",
    "    min_periods = window_length // 2\n",
    "    forward_indexer = FixedForwardWindowIndexer(window_size=window_length)\n",
    "    quantile = (window_length - accepted_outliers - 1) / (window_length - 1)\n",
    "\n",
    "    forward_looking = price.rolling(forward_indexer, min_periods).quantile(quantile, interpolation=\"nearest\").bfill().ffill()\n",
    "    backward_looking = price.rolling(window_length, min_periods, center=False).quantile(quantile, interpolation=\"nearest\").ffill().bfill()\n",
    "\n",
    "    change_points = _get_change_points(price, forward_looking, backward_looking)\n",
    "    return _connect_change_points(change_points, forward_looking)\n"
   ],
   "id": "fb2e60357561149a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def _add_rsp_to_non_promo_df(df_features_non_promo: pd.DataFrame, config) -> pd.DataFrame:\n",
    "    \"\"\"Add retail standard price to non-promo sellout data.\n",
    "\n",
    "    Returns:\n",
    "        A dataframe containing the non-promo sellout data with retail standard price.\n",
    "    \"\"\"\n",
    "    df_features_non_promo = df_features_non_promo.sort_values(by=START_DATE_WEEK)\n",
    "    df_features_non_promo[RETAIL_STANDARD_PRICE] = df_features_non_promo.groupby(config.input_scope.universe_cols, observed=True)[\n",
    "        RAW_AVERAGE_PRICE\n",
    "    ].transform(\n",
    "        lambda x: _infer_rsp_from_non_promo_price(\n",
    "            x,\n",
    "            config.transformation.price_smoothing_window,\n",
    "            config.transformation.accepted_outliers,\n",
    "        )\n",
    "    )\n",
    "\n",
    "    df_features_non_promo[RETAIL_STANDARD_PRICE] = df_features_non_promo.groupby(\n",
    "        config.input_scope.universe_cols,\n",
    "        observed=True,\n",
    "    )[RETAIL_STANDARD_PRICE].bfill()\n",
    "\n",
    "    return df_features_non_promo"
   ],
   "id": "6dc314622449164f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def derive_retail_standard_price(df_features: pd.DataFrame, config) -> pd.DataFrame:\n",
    "    \"\"\"Derive retail standard price from weekly non promo price, then add RSP index.\n",
    "\n",
    "    Retail prices are derived by finding revenue / volume for non promo weeks. It is computed by taking the max price\n",
    "    over the number of promo weeks specified in the `PRICE_SMOOTHING_WINDOW` in the configuration.\n",
    "\n",
    "    Returns:\n",
    "        A dataframe containing the features with retail standard price and RSP index.\n",
    "    \"\"\"\n",
    "    df_features_non_promo = df_features[df_features[PROMO_ON] == 0]\n",
    "    df_features_non_promo_with_rsp = _add_rsp_to_non_promo_df(df_features_non_promo, config)\n",
    "\n",
    "    # match nearest retail standard price in the data\n",
    "    return pd.merge_asof(\n",
    "        df_features.sort_values(START_DATE_WEEK),\n",
    "        df_features_non_promo_with_rsp.sort_values(START_DATE_WEEK)[\n",
    "            [*config.input_scope.universe_cols, START_DATE_WEEK, RETAIL_STANDARD_PRICE]\n",
    "        ],\n",
    "        by=config.input_scope.universe_cols,\n",
    "        on=START_DATE_WEEK,\n",
    "        direction=\"nearest\",\n",
    "    )"
   ],
   "id": "25f941f8884c7a7c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def _update_sellout_promo_event_dates(df_sellout_promo: pd.DataFrame, config) -> pd.DataFrame:\n",
    "    # TODO: document this line\n",
    "    universe_cols = config.input_scope.universe_cols\n",
    "    df_sellout_promo = df_sellout_promo.sort_values(by=[*universe_cols, START_DATE_WEEK])\n",
    "    df_sellout_promo[\"unknown_event_number\"] = (\n",
    "        df_sellout_promo.groupby(universe_cols)[START_DATE_WEEK].diff().dt.days.fillna(10) > 7\n",
    "    ).cumsum()\n",
    "\n",
    "    df_sellout_promo = df_sellout_promo.drop(columns=[PROMO_START_DATE, PROMO_END_DATE])\n",
    "    sellout_promo_start_dates = df_sellout_promo.groupby([*universe_cols, \"unknown_event_number\"], as_index=False).agg(\n",
    "        **{PROMO_START_DATE: (START_DATE_WEEK, \"min\")}  # type: ignore[reportArgumentType]\n",
    "    )\n",
    "    sellout_promo_end_dates = df_sellout_promo.groupby([*universe_cols, \"unknown_event_number\"], as_index=False).agg(\n",
    "        **{PROMO_END_DATE: (START_DATE_WEEK, lambda x: x.max() + pd.DateOffset(days=6))}  # type: ignore[reportArgumentType]\n",
    "    )\n",
    "    df_sellout_promo = df_sellout_promo.merge(sellout_promo_start_dates, on=[*universe_cols, \"unknown_event_number\"], how=\"left\")\n",
    "    return df_sellout_promo.merge(sellout_promo_end_dates, on=[*universe_cols, \"unknown_event_number\"], how=\"left\")"
   ],
   "id": "a9b4a156d36b504c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def _update_sellout_promo_event_values(\n",
    "    df_sellout_promo: pd.DataFrame,\n",
    "    config,\n",
    ") -> pd.DataFrame:\n",
    "    df_sellout_promo[[PROMO_ON, PROMO_EVENT_NAME, PROMO_MECHANIC, PROMO_DEPTH_VALUE]] = (\n",
    "        1,\n",
    "        \"UNKNOWN_EVENT\",\n",
    "        \"Unknown Mechanic\",\n",
    "        config.transformation.sellout_inferred_promo_event_depth,\n",
    "    )\n",
    "    return _update_sellout_promo_event_dates(df_sellout_promo, config)"
   ],
   "id": "7bd89f55d2015f6a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "f14fadbb82cba823"
  },
  {
   "metadata": {
    "collapsed": true
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def infer_missing_promos_from_sellout(df: pd.DataFrame, config) -> pd.DataFrame:\n",
    "    \"\"\"Infer missing promos from sellout.\n",
    "\n",
    "    Some promotions might be missing in the promo calendar.\n",
    "    This functions identifies missing promos if one of these two conditions are met:\n",
    "    - Price diff from RSP > price_inferred_discount_threshold\n",
    "    - Nielsen promo volume > nielsen_promo_volume_share_threshold\n",
    "    price_discount_threshold & nielsen_promo_volume_share_threshold are defined in the configuration as it's OpCo-specific.\n",
    "    \"\"\"\n",
    "    df = derive_retail_standard_price(df, config)\n",
    "    df[\"price_inferred_discount\"] = (df[RETAIL_STANDARD_PRICE] - df[RAW_AVERAGE_PRICE]) / df[RETAIL_STANDARD_PRICE]\n",
    "    df[\"nielsen_promo_volume_share\"] = df[PROMO_VOLUME] / df[VOLUME]\n",
    "\n",
    "    _sellout_inferred_promo_weeks = (\n",
    "        (df[\"price_inferred_discount\"] > config.transformation.missing_promos_inferral.price_inferred_discount_threshold)\n",
    "        | (df[\"nielsen_promo_volume_share\"] > config.transformation.missing_promos_inferral.nielsen_promo_volume_share_threshold)\n",
    "    ) & (df[PROMO_ON] == 0)\n",
    "\n",
    "    df_sellout_promo, df_other = df[_sellout_inferred_promo_weeks], df[~_sellout_inferred_promo_weeks]\n",
    "    df_sellout_promo = _update_sellout_promo_event_values(df_sellout_promo.copy(), config)\n",
    "    df = (\n",
    "        pd.concat([df_sellout_promo, df_other], axis=0)\n",
    "        .sort_values(by=[*config['input_scope']['universe_cols'], START_DATE_WEEK])\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "    return df.drop(columns=[RETAIL_STANDARD_PRICE, \"price_inferred_discount\", \"nielsen_promo_volume_share\", \"unknown_event_number\"])"
   ],
   "id": "initial_id"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "config_HFRA = {\n",
    "    'transformation': {\n",
    "        'price_smoothing_window': 20,\n",
    "        'accepted_outliers': 2,\n",
    "        'sellout_inferred_promo_event_depth': 0.3,\n",
    "        'missing_promos_inferral': {\n",
    "            'price_inferred_discount_threshold': 0.18727472247942986,\n",
    "            'nielsen_promo_volume_share_threshold': 0.6883282528207958\n",
    "        }\n",
    "    },\n",
    "    'input_scope': {\n",
    "        'universe_cols': [\"product_family\", \"customer\"]\n",
    "    }\n",
    "}\n",
    "\n",
    "config_HUK = {\n",
    "    'transformation': {\n",
    "        'price_smoothing_window': 10,\n",
    "        'accepted_outliers': 1,\n",
    "        'sellout_inferred_promo_event_depth': None, # TODO: Verify, originally we have \"null\"\n",
    "        'missing_promos_inferral': {\n",
    "            'price_inferred_discount_threshold': 0.1, \n",
    "            'nielsen_promo_volume_share_threshold': 0.85\n",
    "        }\n",
    "    },\n",
    "    'input_scope': {\n",
    "        'universe_cols': [\"product_family\", \"customer\"]\n",
    "    }\n",
    "}\n",
    "config_map = {\n",
    "    \"HFRA\": config_HFRA,\n",
    "    \"HUK\": config_HUK\n",
    "}"
   ],
   "id": "92d33dd86aff1b13"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# TODO: Please implement below logic and optionally adjust code if needed.\n",
    "\n",
    "df_extracted = \"SQL CODE HERE\"\n",
    "\n",
    "for opco, configuration in config_map.items():\n",
    "    df_preprocessed = infer_missing_promos_from_sellout(df_extracted, configuration)"
   ],
   "id": "e8afb92b90a75a19"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
